#   if(is.null(lib.size)) lib.size <- counts$samples$lib.size*counts$samples$norm.factors
#   counts <- counts$counts
# } else {
#   isExpressionSet <- suppressPackageStartupMessages(is(counts,"ExpressionSet"))
#   if(isExpressionSet) {
#     if(length(Biobase::fData(counts))) out$genes <- Biobase::fData(counts)
#     if(length(Biobase::pData(counts))) out$targets <- Biobase::pData(counts)
#     counts <- Biobase::exprs(counts)
#   } else {
#     counts <- as.matrix(counts)
#   }
# }
#
# #	Check counts
# n <- nrow(counts)
# if(n < 2L) stop("Need at least two genes to fit a mean-variance trend")
# m <- min(counts)
# if(is.na(m)) stop("NA counts not allowed")
# if(m < 0) stop("Negative counts now allowed")
#
#	Check design
# if(is.null(design)) {
#   design <- matrix(1,ncol(counts),1)
#   rownames(design) <- colnames(counts)
#   colnames(design) <- "GrandMean"
# }
#
# #	Check lib.size
# if(is.null(lib.size)) lib.size <- colSums(counts)
#
NormFactors = calcNormFactorsGSD(data.train = data.train, data.test = data.test, method = "deseq")
TrainNormFactor = NormFactors$TrainNormFactor
TestNormFactor = NormFactors$TestNormFactor
TrainLibSize = NormFactors$TrainLibSize
TestLibSize = NormFactors$TestLibSize
lib.size.tr = TrainNormFactor * TrainLibSize
lib.size.ts = TestNormFactor * TestLibSize
#design.tr = model.matrix(~group)
design.tr = matrix(1, ncol(data.train), 1)
rownames(design.tr) = colnames(data.train)
design.ts <- matrix(1, ncol(data.test), 1)
rownames(design.ts) <- colnames(data.test)
colnames(design.ts) <- "GrandMean"
#	Fit linear model to log2-counts-per-million
y.tr <- t(log2(t(data.train + 1)/(lib.size.tr + 1) * 1e+06))
y.ts <- t(log2(t(data.test + 1)/(lib.size.ts + 1) * 1e+06))
y.tr <- y.tr[rownames(y.tr) %in% rownames(data.train), ]  # Extract rows from data
y.ts <- y.ts[rownames(y.ts) %in% rownames(data.train), ]  # Extract rows from data
#y <- normalizeBetweenArrays(y,method="none")
train_dge <- y.tr[keep, ]
test_dge <- y.ts[keep, ]
tr_y <- as.matrix(train_dge)
ts_y <- as.matrix(test_dge)
# tr_cik <- t(tr_y)[, !apply(t(tr_y), 2, function(x) any(x < 0))]
# neg_rem <- colnames(tr_cik)
# ts_cik <- t(ts_y)[,neg_rem]
#
# tr_y <- t(tr_cik)
# ts_y <- t(ts_cik)
#### Variance Filtering ####
ss = apply(tr_y,1,sd)
ort = apply(tr_y,1,mean)
cv = ss/ort
siralama = order(cv, decreasing = TRUE)
fit.tr <- lmFit(tr_y, design.tr,block=NULL,correlation=NULL,weights=weights)
fit.ts <- lmFit(ts_y, design.ts,block=NULL,correlation=NULL,weights=NULL)
if (is.null(fit.tr$Amean))
fit.tr$Amean <- rowMeans(tr_y, na.rm = TRUE)
fit.ts$Amean = fit.tr$Amean
fit.ts$sigma = fit.tr$sigma
fit.ts$coefficients = fit.tr$coefficients[,1]
# #	If no replication found, set all weight to 1
# NWithReps <- sum(fit$df.residual > 0L)
# if(NWithReps < 2L) {
#   if(NWithReps == 0L) warning("The experimental design has no replication. Setting weights to 1.")
#   if(NWithReps == 1L) warning("Only one gene with any replication. Setting weights to 1.")
#   out$E <- y
#   out$weights <- y
#   out$weights[] <- 1
#   out$design <- design
#   if(is.null(out$targets))
#     out$targets <- data.frame(lib.size=lib.size)
#   else
#     out$targets$lib.size <- lib.size
#   return(new("EList",out))
# }
#
#	Fit lowess trend to sqrt-standard-deviations by log-count-size
sx <- fit.tr$Amean + mean(log2(lib.size.tr + 1)) - log2(1e+06)
sy <- sqrt(fit.tr$sigma)
# allzero <- rowSums(counts)==0
# if(any(allzero)) {
#   sx <- sx[!allzero]
#   sy <- sy[!allzero]
# }
l <- lowess(sx,sy,f=span)
# if(plot) {
#   plot(sx,sy,xlab="log2( count size + 0.5 )",ylab="Sqrt( standard deviation )",pch=16,cex=0.25)
#   title("voom: Mean-variance trend")
#   lines(l,col="red")
# }
#	Make interpolating rule
#	Special treatment of zero counts is now removed;
#	instead zero counts get same variance as smallest gene average.
#	l$x <- c(0.5^0.25, l$x)
#	l$x <- c(log2(0.5), l$x)
#	var0 <- var(log2(0.5*1e6/(lib.size+0.5)))^0.25
#	var0 <- max(var0,1e-6)
#	l$y <- c(var0, l$y)
f <- approxfun(l, rule = 2)
#	Find individual quarter-root fitted counts
fitted.values.tr <- fit.tr$coefficients %*% t(fit.tr$design)
fitted.values.ts <- fit.ts$coefficients %*% t(fit.ts$design)
# if(fit$rank < ncol(design)) {
#   j <- fit$pivot[1:fit$rank]
#   fitted.values <- fit$coefficients[,j,drop=FALSE] %*% t(fit$design[,j,drop=FALSE])
# } else {
#   fitted.values <- fit$coefficients %*% t(fit$design)
# }
fitted.cpm.tr <- 2^fitted.values.tr
fitted.cpm.ts <- 2^fitted.values.ts
fitted.count.tr <- 1e-06 * t(t(fitted.cpm.tr) * (lib.size.tr + 1))
fitted.count.ts <- 1e-06 * t(t(fitted.cpm.ts) * (lib.size.ts + 1))
fitted.logcount.tr <- log2(fitted.count.tr)
fitted.logcount.ts <- log2(fitted.count.ts)
#	Apply trend to individual observations
w.tr <- 1/f(fitted.logcount.tr)^4
w.ts <- 1/f(fitted.logcount.ts)^4
dim(w.tr) <- dim(fitted.logcount.tr)
dim(w.ts) <- dim(fitted.logcount.ts)
dimnames(w.tr) = dimnames(tr_y)
dimnames(w.ts) = dimnames(ts_y)
out$E <- tr_y
out$TestExp <- ts_y
out$weights <- w.tr
out$TestWeights <- w.ts
out$design <- design.tr
out$TestDesign <- design.ts
out$Traintargets <- data.frame(lib.size.tr=lib.size.tr)
out$Testtargets <- data.frame(lib.size.ts=lib.size.ts)
out$siralama <- siralama
new("EList", out)
# #	Output
# out$E <- y
# out$weights <- w
# out$design <- design
# if(is.null(out$targets))
#   out$targets <- data.frame(lib.size=lib.size)
# else
#   out$targets$lib.size <- lib.size
# if(save.plot) {
#   out$voom.xy <- list(x=sx,y=sy,xlab="log2( count size + 0.5 )",ylab="Sqrt( standard deviation )")
#   out$voom.line <- l
# }
#
# new("EList",out)
}
voomWithQualityWeights_lasso <- function(data.train, data.test, design=NULL, lib.size=NULL, normalize.method="none", plot=FALSE, span=0.5,
var.design=NULL, var.group=NULL, method="genebygene", maxiter=50, tol=1e-5, trace=FALSE,
col=NULL, ...)
#	Combine voom weights with sample-specific weights estimated by arrayWeights() function for RNA-seq data
#	Matt Ritchie, Cynthia Liu, Gordon Smyth
#	Created 22 Sept 2014. Last modified 7 June 2019.
{
#	Setup side-by-side plots showing (1) the voom trend and (2) the array weights
# if(plot) {
#   oldpar <- par(mfrow=c(1,2))
#   on.exit(par(oldpar))
# }
#	Voom without array weights
v <- voom_lasso(data.train,data.test, design=design, lib.size=lib.size, normalize.method=normalize.method, plot=FALSE, span=span, ...)
#	Estimate array weights on top of voom weights
aw <- limma::arrayWeights(v, design=design, method=method, maxiter=maxiter, tol=tol, var.design=var.design, var.group=var.group)
#	Update voom weights now using the array weights, plotting trend if requested
v <- voom_lasso(data.train,data.test, design=design, weights=aw, lib.size=lib.size, normalize.method=normalize.method, plot=plot, span=span, ...)
#	Update array weights
aw <- limma::arrayWeights(v, design=design, method=method, maxiter=maxiter, tol=tol, trace=trace, var.design=var.design, var.group=var.group)
#	Incorporate the array weights into the voom weights
v$weights <- t(aw * t(v$weights))
v$targets$sample.weights <- aw
#	Plot array weights
# if(plot) {
#   barplot(aw, names=1:length(aw), main="Sample-specific weights", ylab="Weight", xlab="Sample", col=col)
#   abline(h=1, col=2, lty=2)
# }
v
}
#### Preprocessing function ####
# data will be in the split form already
preprocess <- function(data, preProcessing = "deseq-vst", filterGene = FALSE, filterVariance = FALSE, varianceAmount = 1000, ...){
if (is.null(preProcessing)){
cat("Preprocess method not specified. Using the original data without any preprocessing")
return(data)
}
if (preProcessing == "deseq-voom"){
### apply deseq-voom preprocessing steps here
# browser()
train_data <- data@train
test_data <- data@test
tr_status <- train_data$status
ts_status <- test_data$status
df4 <- train_data[, colSums(train_data) == 0]
columns_to_remove <- colnames(df4)
train_data <- subset(train_data, select = !(colnames(train_data) %in% columns_to_remove))
test_data <- subset(test_data, select = !(colnames(test_data) %in% columns_to_remove))
train_time <- train_data$time
test_time <- test_data$time
train_data <- train_data[,-ncol(train_data)]
test_data <- test_data[,-ncol(test_data)]
train_data <- cbind(train_data, tr_status, train_time)
test_data <- cbind(test_data, ts_status, test_time)
colnames(train_data)[colnames(train_data) == "tr_status"] <- "status"
colnames(train_data)[colnames(train_data) == "train_time"] <- "time"
colnames(test_data)[colnames(test_data) == "ts_status"] <- "status"
colnames(test_data)[colnames(test_data) == "test_time"] <- "time"
train_ind <- as.vector(seq(1, nrow(train_data)))
test_ind <- as.vector(seq(max(train_ind)+1, max(train_ind)+nrow(test_data)))
### Extract columns of time and status from train dataset ###
cols_t <- as.vector(colnames(train_data))
cols_train <- cols_t[! cols_t %in% c('time', 'status')]
cols_train <- as.vector(cols_train)
train_d <- subset(train_data, select=cols_train)
### Extract columns of time and status from test dataset ###
cols_te <- as.vector(colnames(test_data))
cols_test <- cols_te[! cols_te %in% c('time', 'status')]
cols_test <- as.vector(cols_test)
test_d <- subset(test_data, select=cols_test)
### Transpose train and test datasets ###
train_matrix <- as.matrix(t(train_d))
test_matrix <- as.matrix(t(test_d))
#### Normalization and transportation ####
norm.method <- "deseq"
group <- c(rep(1,ncol(train_matrix)))
#design = ~1
lib.size = NULL
span = 0.5
transformation <- voomWithQualityWeights_lasso(train_matrix, test_matrix, design=NULL, lib.size=NULL, normalize.method="deseq", plot=FALSE,
span=0.5, var.design=NULL, var.group=NULL, maxiter=50, tol=1e-5,
trace=FALSE, col=NULL, method = "genebygene")
train_E <- transformation$E
train_W <- transformation$targets$sample.weights
test_E <- transformation$TestExp
input_tr <- t(train_E)
input_ts <- t(test_E)
sirala <- transformation$siralama
cv_train <- t(input_tr)[sirala[1:2000],]
cv_test <- t(input_ts)[sirala[1:2000],]
#### Creating final version of train and test datasets ####
# This part needs to be cleaned up
tvsd_train2 <- as.data.frame(cbind(train_data$time, train_data$status, t(cv_train), train_W))
tvsd_test2 <- as.data.frame(cbind(test_data$time, test_data$status, t(cv_test)))
names(tvsd_train2)[names(tvsd_train2) == "V1"] <- "time"
names(tvsd_train2)[names(tvsd_train2) == "V2"] <- "status"
# names(tvsd_train2)[names(tvsd_train2) == "train_ind"] <- "sirano"
names(tvsd_train2)[names(tvsd_train2) == "train_W2"] <- "weight"
tvsd_train2$time <- as.integer(tvsd_train2$time)
tvsd_train2$status <- as.integer(tvsd_train2$status)
# tvsd_train2$sirano <- as.integer(tvsd_train2$sirano)
tvsd_train2[] <- apply(tvsd_train2, 2, function(x) as.double((x)))
names(tvsd_test2)[names(tvsd_test2) == "V1"] <- "time"
names(tvsd_test2)[names(tvsd_test2) == "V2"] <- "status"
# names(tvsd_test2)[names(tvsd_test2) == "test_ind"] <- "sirano"
tvsd_test2$time <- as.integer(tvsd_test2$time)
tvsd_test2$status <- as.integer(tvsd_test2$status)
# tvsd_test2$sirano <- as.integer(tvsd_test2$sirano)
tvsd_test2[] <- apply(tvsd_test2, 2, function(x) as.double((x)))
data@preprocessed_train <- tvsd_train2
data@preprocessed_test <- tvsd_test2
print("Preprocessing is done")
return(data)
}
else if (preProcessing == "deseq-vst"){
### apply deseq-vst preprocessing steps here
# change function names to lasso
train_data <- data@train
test_data <- data@test
cols_t <- as.vector(colnames(train_data))
cols_train <- cols_t[! cols_t %in% c('time', 'status')]
cols_train <- as.vector(cols_train)
train_d <- subset(train_data, select=cols_train)
cols_te <- as.vector(colnames(test_data))
cols_test <- cols_te[! cols_te %in% c('time', 'status')]
cols_test <- as.vector(cols_test)
test_d <- subset(test_data, select=cols_test)
train_matrix <- as.matrix(t(train_d))
test_matrix <- as.matrix(t(test_d))
#### near-zero variance filtering ####
dds_tr <- DESeqDataSetFromMatrix(countData = train_matrix,
colData = as.data.frame(colnames(train_matrix)),
design = ~ 1)
keep_tr <- rowSums(counts(dds_tr)) >= 10
dds_tr <- dds_tr[keep_tr,]
dds_ts <- DESeqDataSetFromMatrix(countData = test_matrix,
colData = as.data.frame(colnames(test_matrix)),
design = ~ 1)
dds_ts <- dds_ts[keep_tr,]
#### train set ####
normalization <- "deseq"
transformation <- "vst"
dataSF_tr <- estimateSizeFactors(dds_tr)    # Estimate Size factors:
dataDisp_tr <- estimateDispersions(dataSF_tr, fitType = "local")  # Estimate dispersions:
transformedData_tr <- varianceStabilizingTransformation(dataDisp_tr, fitType = "local")
dataVST_tr <- t(as.matrix(assay(transformedData_tr)))
input_tr <- dataVST_tr   ## Input data from transformed expression data.
trainParameters_tr <- list(disperFunc = dispersionFunction(dataDisp_tr),
sizeFactors = sizeFactors(dataDisp_tr))
#### test set ####
#Calculation of test set size factors using geometric means from training data
#Genes in the row, samples in the column
geomts = test_matrix / exp(rowMeans(log(train_matrix+0.5)))   ## Geometric mean of test data using estimators from train data
sizeF.ts = apply(geomts, 2, function(x) median(x))
test.dataSF <- estimateSizeFactors(dds_ts)    # Estimate Size factors:
sizeFactors(test.dataSF) <- sizeF.ts             # Replace size factors with size factors which are estimates using train set parameters.
## Change dispersion function of test data with dispersion function of train data
dispersionFunction(test.dataSF) <- trainParameters_tr$disperFunc
transformedData <- varianceStabilizingTransformation(test.dataSF, fitType = "local", blind = FALSE)
dataVST <- t(as.matrix(assay(transformedData)))
input_ts <- dataVST   ## Input data from transformed expression data.
input_tr_nzv <- nearZeroVar(input_tr, saveMetrics = FALSE)
input_tr <- input_tr[,-as.vector(input_tr_nzv)]
input_ts <- input_ts[,-as.vector(input_tr_nzv)]
ss = apply(t(input_tr),1,sd)
ort = apply(t(input_tr),1,mean)
cv = ss/ort
siralama = order(cv, decreasing = TRUE)
cv_train <- t(input_tr)[siralama[1:1900],]
cv_test <- t(input_ts)[siralama[1:1900],]
# tvsd_train2 <- as.data.frame(cbind(train_ind, satir[as.vector(train_ind),][,3:4], t(cv_train)))
# tvsd_test2 <- as.data.frame(cbind(test_ind, satir[as.vector(test_ind),][,3:4], t(cv_test)))
##
tvsd_train2 <- as.data.frame(cbind(data@train$time, data@train$status, t(cv_train)))
tvsd_test2 <- as.data.frame(cbind(data@test$time, data@test$status, t(cv_test)))
names(tvsd_train2)[names(tvsd_train2) == "V1"] <- "time"
names(tvsd_train2)[names(tvsd_train2) == "V2"] <- "status"
# names(tvsd_train2)[names(tvsd_train2) == "train_ind"] <- "sirano"
tvsd_train2$time <- as.integer(tvsd_train2$time)
tvsd_train2$status <- as.integer(tvsd_train2$status)
# tvsd_train2$sirano <- as.integer(tvsd_train2$sirano)
# tvsd_train2$sirano <- as.integer(tvsd_train2$sirano)
tvsd_train2[] <- apply(tvsd_train2, 2, function(x) as.double((x)))
names(tvsd_test2)[names(tvsd_test2) == "V1"] <- "time"
names(tvsd_test2)[names(tvsd_test2) == "V2"] <- "status"
# names(tvsd_test2)[names(tvsd_test2) == "test_ind"] <- "sirano"
tvsd_test2$time <- as.integer(tvsd_test2$time)
tvsd_test2$status <- as.integer(tvsd_test2$status)
# tvsd_test2$sirano <- as.integer(tvsd_test2$sirano)
tvsd_test2[] <- apply(tvsd_test2, 2, function(x) as.double((x)))
# Merge all the relevant data into a single S4 object
data@preprocessed_train <- tvsd_train2
data@preprocessed_test <- tvsd_test2
browser()
cat("Preprocessing is done.\n")
return(data)
}
else {
cat("Available preprocessing methods are: \"deseq-vst\" and \"deseq-voom\".")
stop()
}
}
survival.rfsrc <- function(data = data, method = "rfsrc", fsParams, trainParams, tuneGrid, ...){
# this function might return an S4 object as data
# S4 object could contain: train data set, test data set, time column, event column, original data
browser()
data <- preprocess(data, "deseq-vst")
# double check if this actually works.
learner <- lrn("surv.rfsrc", id = method, ...) # will take parameters dynamically
task_fs = TaskSurv$new("task_fs", data@preprocessed_train, time = "time", event = "status")
measures = msrs(c("surv.cindex")) # this probably should change with a parameter.
# measures = msrs(c("survival.cindex"))
# fsParams is a list of lists that contain appropriate parameters for feature selection. These should be available in man page for the function.
# First element of this list is notable since it can/will contain multiple elements.
# May need to look into unlisting them from a list format.
# We can't simply dedicate more elements of the list because the list will contain varying amount of elements depending on the method chosen.
instance = FSelectInstanceSingleCrit$new(
task = task_fs,
learner = learner,
resampling = do.call(mlr3::rsmp, fsParams[[1]]),
measure = do.call(mlr3::msr, fsParams[[2]]),
terminator = do.call(mlr3verse::trm, fsParams[[3]])
)
fselector = do.call(mlr3verse::fs, fsParams[[4]])
fselector$optimize(instance)
#
# browser()
features <- as.vector(instance$result_feature_set)
data@preprocessed_train <- cbind(subset(data@preprocessed_train, select=features), data@preprocessed_train$time, data@preprocessed_train$status) # this needs to be cleaned up and be more dynamic
data@preprocessed_test <- cbind(subset(data@preprocessed_test, select=features), data@preprocessed_test$time, data@preprocessed_test$status)
# names of the column need to be fixed
names(data@preprocessed_train)[names(data@preprocessed_train) == "data@preprocessed_train$time"] <- "time"
names(data@preprocessed_train)[names(data@preprocessed_train) == "data@preprocessed_train$status"] <- "status"
names(data@preprocessed_test)[names(data@preprocessed_test) == "data@preprocessed_test$time"] <- "time"
names(data@preprocessed_test)[names(data@preprocessed_test) == "data@preprocessed_test$status"] <- "status"
task_tune <- TaskSurv$new("task_tune", data@preprocessed_train, time = "time", event = "status")
# browser()
tune_space <- ParamSet$new(tuneGrid)
at=AutoTuner$new(learner=learner,
resampling=do.call(mlr3::rsmp, trainParams[[1]]),
measure = do.call(mlr3::msr, trainParams[[2]]),
terminator = do.call(mlr3verse::trm, trainParams[[3]]),
tuner=do.call(mlr3verse::tnr, trainParams[[4]]),
search_space = tune_space
)
at$train(task_tune)
data@model <- at
pred_result <- at$predict_newdata(newdata = data@preprocessed_train)$score(measures)
data@cindex_train <- pred_result
return(data)
}
result_rfsrc <- survival(data = data_obj, method = "rfsrc",
fsParams = list(list("repeated_cv", repeats = 1, folds = 2), list("surv.cindex"), list("evals", n_evals = 3), list("random_search")),
trainParams = list(list("repeated_cv", repeats = 2, folds = 2), list("surv.cindex"), list("evals", n_evals = 3), list("random_search")),
tuneGrid = param_list)
data@train
View(data@preprocessed_test)
survPredict.MLR <- function(model){
time_grid <- sort(unique(model@preprocessed_train$time[model@preprocessed_train$status == 1]))
pred_result <- model@model$predict_newdata(newdata = model@preprocessed_test)
cindex_test <- pred_result$score(msrs(c("surv.cindex")))
survHazards <- 1 - pred_result$distr$pdf(time_grid)
model@cindex_test <- cindex_test
model@survHazards <- t(survHazards)
model@times <- time_grid
return(model)
}
resultMLR <- survPredict(result_rfsrc)
View(resultMLR@survHazards)
survPredict.MLR <- function(model){
time_grid <- sort(unique(model@preprocessed_train$time[model@preprocessed_train$status == 1]))
pred_result <- model@model$predict_newdata(newdata = model@preprocessed_test)
cindex_test <- pred_result$score(msrs(c("surv.cindex")))
survHazards <- 1 - pred_result$distr$cdf(time_grid)
model@cindex_test <- cindex_test
model@survHazards <- t(survHazards)
model@times <- time_grid
return(model)
}
survPredict.MLR <- function(model){
browser()
time_grid <- sort(unique(model@preprocessed_train$time[model@preprocessed_train$status == 1]))
pred_result <- model@model$predict_newdata(newdata = model@preprocessed_test)
cindex_test <- pred_result$score(msrs(c("surv.cindex")))
survHazards <- 1 - pred_result$distr$cdf(time_grid)
model@cindex_test <- cindex_test
model@survHazards <- t(survHazards)
model@times <- time_grid
return(model)
}
resultMLR <- survPredict(result_rfsrc)
autoplot(pred_result)
?autoplot.PredictionSurv
autoplot.PredictionSurv(pred_result)
mlr3proba:::autoplot.PredictionSurv(pred_result)
mlr3proba:::autoplot.PredictionSurv(pred_result, type = "preds")
autoplot.PredictionSurv
survPlot(resultMLR)
fixInNamespace("autoplot.PredictionSurv", ns = "mlr3proba")
fixInNamespace("gprm", ns = "distr6")
resultMLR <- survPredict(result_rfsrc)
model@model
View(model@model)
?model@model$predict_newdata
pred_result$help()
pred_result$print()
View(pred_result$print())
test_df <-pred_result$print()
test_df
View(test_df)
View(test_df$distr)
View(test_df$distr[1])
View(test_df$distr[[1]])
View(test_df$distr[[1]][[1]])
pred_result$distr
View(pred_result$print())
View(resultMLR@preprocessed_test)
View(resultPL@preprocessed_test)
model@model$predict_newdata
paramListConstructor <- function(df){
param_list <- list()
for (i in 1:nrow(df)){
row <- df[i,]
if (row$type == "numeric"){
param_list <- c(param_list, ParamDbl$new(row$paramid[[1]], lower = row$lower[[1]], upper = row$upper[[1]]))
}
else if (row$type == "character"){
param_list <- c(param_list, ParamFct$new(row$paramid[[1]], levels = row$levels[[1]]))
}
else if (row$type == "integer"){
param_list <- c(param_list, ParamInt$new(row$paramid[[1]], lower = row$lower[[1]], upper = row$upper[[1]]))
}
else if (row$type == "logical"){
param_list <- c(param_list, ParamLgl$new(row$paramid[[1]]))
}
else{
stop(paste0("The parameter type ", row$type, " is not supported"))
}
}
return(param_list)
}
exampleParamDf <- function(){
row1 <- c("numeric", "nu", 0, 1)
example_df <- data.frame(type = row1[1], paramid = row1[2], lower = row1[3], upper = row1[4])
example_df <- rbind(test, row1)
example_df <- rbind(test, row1)
example_df <- rbind(test, row1)
example_df$levels <- list(NA, c("coxph","weibull"), NA, NA)
example_df$type <- list("numeric", "character", "integer", "logical")
example_df$paramid <- list("mtry.ratio","splitrule","ntree","membership")
example_df$lower <- list(0, NA, 500, NA)
example_df$upper <- list(0.7, NA, 1500, NA)
example_df$levels <- list(NA, c("logrank","bs.gradient"), NA, NA)
return(example_df)
}
